# ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS

# Abstract
 - 자연어 처리 작업에서 사전 교육을 할 때 모델의 크기를 늘리면 다운스트림 작업에서 성능이 향상되는 경우가 많음
 - 하지만 GPU/TPU  메모리 제한과 긴 학습 시간 때문에 어느 시점부터는 모델의 크기를 증가 시키는 것의 어려움이 있음
 - 이런 문제를 해결하기 위해 메모리 소비를 낮추고 BERT의 훈련 속도를 높이기 위한 두가지 파라미터 감소 기법을 제시
 - 제안된 방법이 BERT에 비해 훨씬 더 잘 확장되는 모델로 이어진다는 것을 보여줌
 - 문장 간의 일관성을 모델링 하는데 초점을 맞추고 다중 문장 입력으로 다운스트림 작업에 일관되게 도움이 되는 SELF-SUPERVISED LOSS 사용
 - BERT-LARGE에 비해 파라미터가 적으면서 GLUE,RACE,SQuAD 벤치마크에서 SOTA를 달성

# Introduction
 - 사전 학습은 언어 표현 학습에서 좋은 성능을 내고 있음
 - RACE테스트에서 제시한 44.1%보다 45.3%가 높은 고성능 사전 학습된 언어 표현을 모델의 향상률을 보여줌 
 - 이런 증거는 대규모 네트워크가 최첨단 성능을 달성하는데 매우 중요하다는 것을 보여줌
 - 대형 모델로 사전 교육하고 작은 모델로 증류하는것이 일반적 관행
 - 더 나은 NLP모델을 갖는 것이 더 큰 모델을 갖는 것 만큼 쉬운 일인가의 답변은 사용가능한 하드웨어의 메모리 제한을 드러냄
 - 최신 모델의 파라미터가 수억개 또는 수십억개인 경우가 많기 때문에 모델을 확장할때에 이런 한계에 부딪히기 쉬움
 - communication overhead(생산적인 작업을 수행하는 대신 팀과 커뮤니케이션하는 데 소비하는 시간의 비율)는 모델의 파라미터 수에 정비례하기 때문에 분산 학습에서도 학습속도에 크게 방해될 수 있음
 - 앞에서 언급한 문제에 대한 솔루션으로는 모델의 병렬화랑 메모리 관리가 있음
 - 이런 솔루션은 메모리 제한 문제를 해결하지만 communication overhead는 해결 못함
 - 논문에서는 기존의 BERT 아키텍처보다 파라미터가 훨씬 적은 A Lite BERT(ALBERT)아키텍처를 설계하여 앞에서 언급한 모든 문제 해결
 - ALBERT는 모델의 크기에 주요 장애물을 줄이는 두가지 파라미터 변수 감소 기술을 통합
    1. 분리된 임베딩 매개변수화
       - 큰 어휘 임베딩 행렬을 두개의 작은 행렬로 분해하여 hidden layer의 크기와 어휘 임베딩 크기를 분리
       - 분리를 통해 어휘 임베딩의 파라미처 크기를 크게 늘리지 않고도 hidden size를 쉽게 확장 가능
    2. 교차 계층 파라미터 공유
       - 네트워크 깊이와 함께 파라미터가 커지는 것을 방지
 
 - 두 기법 모두 성능을 크게 손상시키지 않고 BERT에 대한 파라미터 수를 크게 줄여 파라미터 효율성을 향상
 - BERT-large와 유사한 ALBERT 구성은 파라미터 변수가 18배 더 적으며 약 1.7배 더 빠르게 훈련 가능
 - 파라미터 감소 기술은 훈련을 안정시키고 일반화에 도움을 주는 정규화의 한 형태로도 작용
 - ALBERT의 성능을 더욱 향상시키기 위해 sentence-order prediction(SOP)에 대한 self-supervised도 도입
 - SOP는 문장 간 일관성에 중심을 두고 원래 BERT에서 제안된 NSP 손실의 비효율성을 해결하도록 설계
 - 설계의 결과로 BERT-Large보다 파라미터가 적지만 훨씬 더 나은 성능을 달성하는 더 큰 ALBERT로 확장 할 수 있음
 - GLUE,SQuAD,RACE 벤치마크에 대한 SOTA결과 수립


# RELATED WORK

 <h3>Scaling Up Representation Learning For Natural Language</h3>
 
 - 자연어의 학습 표현은 NLP 작업에 유용한 것으로 나타나며 널리 채택됨
 - 지난 2년 동안 가장 중요한 변화 중 하나는 표준 또는 상황에 맞는 사전 학습에서 전체 네트워크 사전 학습 다음에 작업별 미세 조정으로 전환하는 것
 - 이런 작업에서 모델 크기가 클수록 성능이 향상됨이 나타남
 - 자연어 이해 작업에서 hidden size,hidden layer,attention head가 더 많은 것이 더 좋은 성능을 나타냄
 - 하지만 모델 크기와 계산 비용 문제에서 hidden size를 1024에서 멈춤
 - 특히 GPU/TPU 메모리 제한 측면에서 계산 제약으로 인해 대형 모델로 실험하기 어려움
 - 최신 모델들이 수 억단위에 파라미터를 가진 것을 고려하면 쉽게 메모리 제한에 도달할 수 있음
 - 이 문제를 해결하기 위해 과거에는 추가 순방향 패스의 비용으로 하위 선형 메모리 요구 사항을 줄이기 위해 그래디언트 체크포인트라고 하는 방법을 제안,중간 활성화를 저장할 필요 없도록 다음 계층에서 각 계층의 활성화를 재구성하는 방법 제안,모델 병렬화를 사용할 것을 제안
 - 대조적으로 본 논문의 파라미터 감소 기술은 메모리 소비를 줄이고 훈련 속도를 증가 시킴

 <h3>Cross-Layer Parameter Sharing</h3>
 
 - Layer간 파라미터를 공유하는 아이디어는 transformer 아키텍처를 통해 탐구 되었지만 이 연구는 사전 학습/미세조정보다는 표준 인코더-디코더 작업에 대한 훈련에 초점을 둠
 - cross-layer parameter를 가진 네트워크가 표준 transformer보다 언어 모델링 및 주제-동사 수일치에서 보다 더 나은 성능을 보임
 - 최근 transformer 네트워크를 위한 심층 평형 모델(DQE)을 제안하고 DQE가 특정 layer의 입력 임베딩과 출력 임베딩이 동일한 평형점에 도달할 수 있음을 보여줌
 - 임베딩이 수렴하기보다는 진동하다는 것을 보여줌
 - 파라미터 공유 transformer와 표준 transformer를 결합하여 표준 transformer의 파라미터 수를 더욱 증가 시킴

 <h3>Sentence Ordering Objectives</h3>
 
 - Albert는 텍스트의 연속된 두 세그먼트의 순서를 예측하는 것에 기초한 사전 학습 손실을 사용
 - Skip-thought와 FastSent 문장 임베딩은 문장의 인코딩을 사용하여 인접 문장의 단어를 예측함으로써 학습됨
 - 문장 임베딩 학습의 다른 목표로는 근처 이웃들만이 아닌 미래 문장 예측과 명시적 담화 표지어 예측이 있음
 - 논문의 loss는 두개의 연속된 문장의 순서를 결정하기 위해 문장 임베딩을 학습하는 문서 순서 목표와 유사
 - 대부분의 작업과 달리 논문의 loss는 문장보다 텍스트 세그먼트에 정의됨
 - BERT는 쌍의 두번째 세그먼트가 다른 문서의 세그먼트와 스왑 되었는지 여부를 예측하여 loss를 사용함
 - 실험에서 이러한 loss을 비교하고 sentence ordering이 더 어려운 사전 학습 작업이며 특정 다운스트림 작업에 더 유용하다는 것을 발견
 
# The Elements of Albert

 - Albert에 대한 설계 제시 원본 BERT 아키텍처의 해당 구성에 대한 비교를 제공

 <h3>Model Architecture Choices</h3>
  
  - Albert 아키텍처의 뼈대는 GELU의 비선형성을 가진 Transformer의 인코더를 사용(BERT와 유사)
  - BERT의 표기 규칙을 따르며, 임베딩의 크기를 E, 인코더 layer의 수를 L, 숨겨진 크기(hidden size)를 H로 표시
  - 이어서 feed-forward/filter 크기를 4H로, attention head의 수를 H/64로 설정
  - BERT의 설계 선택에 대해 Albert는 세 가지 주요 기여 존재

    <h5>Factorized embedding parameterization</h5>
 
 
