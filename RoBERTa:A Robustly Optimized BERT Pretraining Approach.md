# RoBERTa:A Robustly Optimized BERT Pretraining Approach
- Yinhan Liu Myle Ott Naman Goyal Jingfei Du Mandar Joshi  Danqi Chen  Omer Levy  Mike Lewis  Luke Zettlemoyer Veselin Stoyanov 
# Abstract
 - 많은 주요 하이퍼 파라미터와 훈련 데이터 크기의 영향을 신중하게 측정하는 BERT 사전 훈련의 복제 연구 제시
 - 지금의 BERT보다 더 많은 데이터에 훈련되고 이전에 간과된 설계 선택의 중요성을 강조

# Introduction
 - ELMO,GPT,BERT,XLM,XLNet과 같은 사전 훈련 모델들은 상당한 성능 향상을 가져왔지만 어떤 측면이 가장 많이 기여하는지 결정하는 것은 쉽지 않음
 - 훈련은 계산 비용이 많이 들기 때문에 수행할 수 있는 튜닝 양을 제한하고,종종 다양한 크기의 개인 훈련 데이터로 수행되며,모델링 진보의 영향을 측정할 수 있는 능력 제한
 - 본 논문에서는 하이터 파라미터 튜닝과 훈련 세트 크기의 영향에 대한 신중한 평가를 포함하는 BERT 사전 훈련의 복제 연구를 제시
 - RoBERTA라는 BERT모델을 훈련하기 위한 개선된 방법을 제시
 
       1) 더 큰 배치로 모델을 더 오래 훈련시키고,더 많은 데이터를 더 많이 훈련 시킴
       2) NSP(Next Sentence Prediction)목표를 제거
       3) 더 긴 시퀀스에 대한 훈련
       4) 훈련 데이터에 적용되는 마스킹 패턴을 동적으로 변경
       5) 대규모 새 데이터 세트(CC-NEWS)를 수집

 - BERT의 마스크 언어 모델 훈련 목표가 autoregressive language modeling과 같이 최근에 제안된 다른 훈련 목표와 경쟁한다는 것을 다시 설정
 - 중요한 BERT 설계 선택 및 훈련 전략을 제시,더 나은 다운스트림 작업 성능을 이끌어낼 대안 소개
 - 사전 훈련을 위해 더 많은 데이터를 사용하는 것이 다운스트림 작업에서 성능을 더 향상시킨다는 것을 확인
 - 훈련 개선은 올바른 설계 선택 하에서 마스크된 언어 모델 사전 훈련이 최근 발표된 다른 모든 방법과 경쟁적임을 보여줌

# BackGround
 - BERT의 사전 훈련 접근 방법 소개

   # Setup
    - BERT는 입력으로 두개의 segments의 연결을 취함
    - segments는 일반적으로 둘 이상의 자연어 문장으로 구성
    - 두 segments는 [CLS] x1,x2...xN,[SEP] y1,y2...yM,[EOS]로 구분되는 특수 토큰과 함께 BERT에 단일 입력 시퀀스로 표시됨
    - M과 N은 M+N<T로 제한,T는 훈련 중 최대 시퀀스 길이를 제어하는 매개변수
    - 모델은 먼저 레이블이 지정되니 않은 대형 텍스트 말뭉치에서 사전 훈련되고 이후 최종 작업 레이블 데이터를 사용하여 미세 조정됨
  
   # Architecture
    - BERT는 현재 transformer 아키텍처를 사용하며, 본 연구에서는 L layer와 함께 transformer 구조를 사용할 것
    - 각 블록은 A self-attention heads와 H hidden dimension을 사용

   # Training Objectives
    - 사전 교욱 중에 BERT는 다음 두가지 목표를 사용
    - Masked Language Model(MLM)
       - 입력 시퀀스에 있는 토큰의 랜덤 샘플이 선택 되고 특수 토큰 [MASK]로 대체
       - MLM의 목표는 마스킹된 토큰을 예측할 때 Cross-entropy loss
       - BERT는 교체를 위해 토큰의 15%를 균일하게 선택
       - 선택한 토큰 중 80%는 [MASK]로 대체,10%는 변경하지않고 그대로 유지,10%는 임의로 선택한 어휘 토큰으로 대체
       - 원래 실험에서는 랜덤 마스킹과 교체는 처음에 한번 수행되고 훈련 기간 동안 저장되지만, 실제로는 데이터가 중복되어 모든 훈련 문장에 대해 마스크가 항상 동일하지는 않음
    - Next Sentence Prediction(NSP)
       - NSP는 두개의 segments가 원본 텍스트에서 서로 따르는지 여부를 예측하기 위한 binary classification loss
       - NSP의 목표는 문장 쌍 간의 관계에 대한 추론이 필요한 자연어 추론과 같은 다운스트림 작업에서 성능을 향상시키기 위해 설계
   
   # Optimization
    - BERT는 Adam과 함께 β1 = 0.9, β2 = 0.999, ǫ = 1e-6 and L2 weight decay of 0.01의 매개 변수를 사용하여 최적화
    - 학습률은 처음 10000단계에 걸쳐 1e-4의 피크 값으로 워밍업 된 다음 선형적으로 소멸
    - BERT는 모든 layer와 attention 가중치에 대해 0.1의 dropout과 GELU 활성화 함수로 훈련함
    - 모델은 최대 길이 T=512토큰,B=256 시퀀스를 포함하는 미니배치,1000000개의 업데이트를 위해 사전 교육을 받음
   # Data
    - BERT는 BOOKCORPUS와 영어 위키백과의 조합에 대해 교육,16GB의 텍스트

# Experimental Setup
 - BERT 복제 연구를 위한 실험 설정에 대해 설명
   
   # Implementation
    - FAIRSEQ에서 BERT를 다시 구현
    - 별도로 조정되는 최고 학습률과 워밍업 단계의 수를 제외하고 원래 BERT 최적화 하이터 매개변수를 주로 따름
    - Adam epsilon에 민감하다는 것을 발견,이를 조정한 후 더 나은 성능,안정성을 얻음
    - 큰 배치크기로 훈련할 때 안정성을 향상 시키기 위해 β2 = 0.98로 설정
    - 최대 T = 512 토큰의 시퀀스로 사전 훈련
    - 무작위로 짧은 시퀀스를 주입하지 않으며, 업데이트의 처음 90%에 대해 시퀀스 길이를 줄인 상태로 훈련하지 않음
   # Data
    - BERT 스타일의 사전 훈련은 결정적으로 많은 양의 텍스트에 의존
    - 연구를 위해 실험을 위해 가능한 한 많은 데이터를 수집하는 데 초점을 맞춤
      - BOOKCORPUS (Zhu 등, 2015)와 영어 위키백과를 사용한다. 이 데이터는 BERT(16GB)를 교육하는 데 사용되는 원본 데이터
      - Common Crawing News 데이터 세트의 영어 부분에서 수집한 CC-NEWS(Nagel, 2016). 자료에는 2016년 9월부터 2019년 2월까지 기어들어온 6300만 건의 영문 뉴스 기사(필터링 후 76GB)
      - OPENWEBTEXT 이 텍스트는 Reddit에서 공유된 URL에서 추출된 웹 콘텐츠로 적어도 세 개의 업보트가 존재(38GB)
      - STORES는 Winograd 스키마의 스토리 스타일과 일치하도록 필터링된 CommonCraw 데이터의 하위 집합을 포함하는 데이터 세트이다. (31GB)

   # Evaluation
    - 다음 세 가지 벤치마크를 사용하여 다운스트림 작업에 대해 사전 훈련된 모델을 평가
    - GLUE
      - 자연 언어 이해 시스템을 평가하기 위한 9개 데이터 세트의 모음
      - 작업은 단일 문장 분류 또는 문장 쌍 분류 작업으로 프레임화
    - SQuAD
      - 문맥과 질문을 제공
      - SQuAD의 두 가지 버전인 V1.1과 V2.0을 평가
      - V1.1에서 컨텍스트는 항상 답변을 포함하는 반면 V2.0에서는 제공된 컨텍스트에서 일부 질문이 답변되지 않아 작업이 더 어려움
      - SQuAD V1.1의 경우 BERT와 동일한 스팬 예측 방법을 채택
      - SQuAD V2.0의 경우 질문이 대답 가능한지 여부를 예측하기 위해 이진 분류기를 추가
    - RACE
      - 28,000개 이상의 구절과 거의 100,000개의 질문을 가진 대규모 읽기 이해 데이터 세트
      - 중고등학생을 위해 설계된 중국의 영어 시험에서 수집
      - 모든 질문에 대해 네 가지 옵션 중에서 하나의 정답을 선택하는 것이 과제
      - 다른 인기 있는 읽기 이해 데이터 세트에 비해 문맥이 상당히 길고 추론을 필요로 하는 질문의 비율이 매우 큼
 
# Training Procedure Analysis
 - BERT 모델을 성공적으로 사전 교육하는 데 중요한 선택 사항을 탐색하고 정량화
 - 모델 아키텍처를 고정 상태로 유지
 - BERTBASE와 동일한 구성의 BERT 모델을 교육하는 것으로 시작(L = 12, H = 768, A = 12, 110M 매개 변수).

   # Static vs Dynamic Masking
    - 기존 BERT 구현은 데이터 사전 처리 중에 마스킹을 한 번 수행하여 단일 정적 마스크를 생성
    - 모든 시기에서 각 훈련 인스턴스에 대해 동일한 마스크를 사용하지 않도록 훈련 데이터를 10회 복제하여 각 시퀀스를 40회 동안 10가지 방법으로 마스킹
    - 따라서 각 훈련 시퀀스는 훈련 중에 동일한 마스크로 4번 관찰
    - 이 전략을 모델에 시퀀스를 공급할 때마다 마스킹 패턴을 생성하는 동적 마스킹과 비교
    - 이는 더 많은 단계 또는 더 큰 데이터 세트를 위해 사전 훈련할 때 중요해짐
    
    ![image](https://user-images.githubusercontent.com/70500214/110244444-06c5fc00-7fa2-11eb-9067-59a685e7fce8.png)
    
    - BERTBASE 결과를 정적 또는 동적 마스킹으로 재실행과 비교
    - 동적 마스킹이 정적 마스킹 보다 약간 더 우수하므로 나머시 실험에서 동적 마스킹을 사용
  
   # Model Input Format and Next Sentence Prediction
    - 기존 BERT 사전 교육 절차에서 모델은 동일한 문서(p = 0.5 포함) 또는 별개의 문서에서 연속적으로 샘플링되는 두 개의 연결된 문서 세그먼트를 관찰
    - 마스킹 언어 모델링 목표 외에도, 모델은 관찰된 문서 segment가 보조 NSP(Next Presentation) 손실을 통해 동일하거나 별개의 문서에서 오는지 여부를 예측하도록 훈련
    - NSP 손실은 원래 BERT 모델을 훈련하는 데 중요한 요소라는 가설을 세움
    - NSP를 제거하면 QNLI, MNLI 및 SQuAD 1.1에서 성능이 크게 저하되어 성능이 저하되는 것을 관찰
    - NSP 손실의 필요성에 의문을 제기
    - 이러한 불 필요성을 더 잘 이해하기 위해, 몇 가지 대안적 훈련 형식
    - SEGMENT-PAIR+NSP
       - BERT(Devlin et al., 2019)에서 사용된 원래 입력 형식과 NSP 손실을 비교
       - 각 입력에는 여러 개의 자연 문장을 포함할 수 있는 한 쌍의 세그먼트가 있지만, 결합된 총 길이는 512개의 토큰보다 작아야 함
    - SENTENCE-PAIR+NSP
       - 각 입력은 한 문서의 연속적인 부분 또는 별도의 문서에서 샘플링된 한 쌍의 자연 문장을 포함
       - 이러한 입력은 512개의 토큰보다 상당히 짧기 때문에 전체 토큰 수가 SEGMENT-Pair+NSP와 유사하게 유지되도록 배치 크기를 늘림(NSP 손실은 그대로 유지)
    - FULL-SENTENCES
       - 각 입력은 하나 이상의 문서에서 연속적으로 샘플링된 전체 문장으로 채워져 있으며, 총 길이는 최대 512개의 토큰
       - 입력이 문서 경계를 넘을 수 있음
       - 한 문서의 끝에 도달하면 다음 문서에서 문장을 샘플링하기 시작하고 문서 사이에 구분자 토큰을 추가(NSP 손실을 제거)
    - DOC-SENTENCES
       -  입력은 문서 경계를 넘을 수 없다는 점을 제외하고 FULL-SENTENCES과 유사하게 구성
       -  문서 끝 근처에 샘플링된 입력은 512개의 토큰보다 짧을 수 있으므로, 우리는 이러한 경우 배치 크기를 동적으로 늘려 FULL SENTENCES와 비슷한 수의 총 토큰을 달성(NSP 손실을 제거)
      
      ![image](https://user-images.githubusercontent.com/70500214/110244870-1f371600-7fa4-11eb-9732-52f391c0778a.png)
    
    - 네 가지 설정에 대한 결과를 보여줌
    - SEGMENT-Pair 입력 형식을 SENTENCE-Pair 형식으로 비교
    - 두 형식 모두 NSP 손실을 유지하지만 후자는 단일 문장을 사용
    - 우리는 개별 문장을 사용하면 다운스트림 작업의 성능이 저하된다는 것을 발견,이는 모델이 장거리 종속성을 배울 수 없기 때문이라고 가정
    - FULL-SENTENCES와 DOC-SENTENCES 비교
    - NSP 손실을 제거하면 다운스트림 작업 성능이 일치하거나 약간 향상된다는 것을 발견
    - 단일 문서에서 나오는 시퀀스(DOC-SENTENCE)를 제한하는 것이 여러 문서의 시퀀스(FULL-SENTENCE)를 포장하는 것보다 약간 더 낫다는 것을 발견
    - 그러나 DOC-SENTENCES 형식은 다양한 배치 크기를 낳기 때문에 관련 작업과 쉽게 비교하기 위해 실험의 나머지 부분에서 FULL SENTESSENTES를 사용

   # Training with large batches
    - 과거 Neural Machine Translation 연구는 학습 속도가 적절히 증가했을 때 매우 큰 미니 배치를 사용하는 훈련이 최적화 속도와 최종 작업 성능을 모두 향상시킬 수 있다는 것을 보여줌
    - 최근 연구는 BERT가 대규모 배치 교육에도 적합하다는 것을 보여줌
    - 원래 256개의 시퀀스의 배치 크기를 가진 1M 스텝에 대해 BERTBASE를 훈련, 이는 그레이디언트 축적을 통해 배치 크기가 2K인 125K 단계 또는 배치 크기가 8K인 31K 단계에 대한 훈련과 동일한 계산 비용
   
   ![image](https://user-images.githubusercontent.com/70500214/110245143-5f4ac880-7fa5-11eb-8a5f-8bc67995884c.png)

    - 배치 크기를 증가시키고 훈련 데이터를 통과하는 패스 수를 제어하면서 BERTBASE의 복잡성과 최종 작업 성능을 비교
    - 대규모 배치를 사용한 훈련이 최종 작업 정확도와 함께 마스크된 언어 모델링 목표에 대한 복잡성을 개선한다는 것을 관찰
    - 대규모 배치는 분산 데이터 병렬 훈련을 통해 병렬화가 더 쉬우며,이후 실험에서 우리는 8K 시퀀스의 배치를 사용하여 훈련

   # Text Encoding
    - BPE(Byte-Pair Encoding)는 자연어 코퍼스에서 공통적으로 사용되는 큰 어휘를 처리할 수 있는 문자 및 단어 수준 표현 간의 하이브리드이다.
    - 전체 단어 대신 BPE는 하위 단어 단위에 의존하며, 이는 훈련 말뭉치의 통계 분석을 수행하여 추출
    - BPE 어휘 크기는 일반적으로 10K-100K 서브워드 단위
    - 유니코드 문자는 본 연구에서 고려된 것과 같이 크고 다양한 코퍼라를 모델링할 때 이 어휘의 상당한 부분을 차지할 수 있음
    - 유니코드 문자 대신 바이트를 기본 하위 단어 단위로 사용하는 BPE의 영리한 구현을 소개
    - 바이트를 사용하면 "알 수 없는" 토큰을 도입하지 않고도 입력 텍스트를 인코딩할 수 있는 중간 크기(50K 단위)의 하위 단어 어휘를 학습할 수 있음
    - 원래의 BERT 구현은 30K 크기의 문자 수준 BPE 어휘를 사용하며, 이는 경험적 토큰화 규칙으로 입력을 사전 처리한 후 학습
    - 이어 추가 사전 처리 또는 입력 토큰화 없이 50K 하위 단어 단위를 포함하는 더 큰 바이트 수준 BPE 어휘로 BERT를 교육하는 것을 고려
    - 이렇게 하면 BERTBASE 및 BERTLARGE에 각각 약 15M 및 20M 파라미터가 추가됨
    - BPE가 일부 작업에서 약간 더 나쁜 최종 작업 성능을 달성하면서 이러한 인코딩 간에 약간의 차이만 나타냄
    - 범용 인코딩 방식의 이점이 성능에서 사소한 저하보다 크다고 믿고 나머지 실험에서 이 인코딩을 사용

# RoBERTa
 - 개선 사항을 종합하여 결합된 영향을 평가
 - 이 구성을 Robustly optimized BERT approach줄여서 RoBERTa라고 부름
 - RoBERTA는 동적 마스킹, NSP 손실 없는 FULL-SENTENCES , 대형 미니 배치 및 더 큰 바이트 레벨 BPE로 훈련
 - 이전 연구에서 강조되지 않았던 두 가지 다른 중요한 요소 : 사전 훈련에 사용된 데이터와 훈련 횟수가 데이터를 통과한다는 두 가지 중요한 요인을 조사
 - BERT LARGE 아키텍처(L = 24, H = 1024, A = 16, 355M 매개 변수)에 따라 RoBERTA를 교육하는 것으로 시작
 
 ![image](https://user-images.githubusercontent.com/70500214/110246003-26acee00-7fa9-11eb-8a13-faeec67f326d.png)
 
 - RoBERTA가 당초 보고된 BERTLARGE 결과에 비해 크게 개선
 - 세 개의 추가 데이터 세트와 결합
 - RoBERTa를 훨씬 더 오래 사전 훈련하여 사전 훈련 단계 수를 100K에서 300K로 증가시키고, 나아가 500K로 증가
 - 대부분의 작업에서 XLNetLARGE를 능가
 - 가장 오래 훈련된 모델도 오버피팅이 일어나지 않음을 알 수 있음
   
   # GLUE Results
    - 두 가지 미세 조정 설정을 고려
    - 첫 번째 설정(single-task, dev)에서는 해당 작업에 대한 교육 데이터만 사용하여 GLUE 작업 각각에 대해 RoBERTA를 별도로 미세 조정
    - 배치 크기 α {16, 32} 및 학습 속도 β {1e-5, 2e-5, 3e-5}인 각 작업에 대해 제한된 하이퍼 파라미터 스위프를 고려,10 epoch, early stopping
    - 나머지 하이퍼 파라미터는 사전 훈련 때와 동일하게 유지
    - 모델 앙상블 없이 다섯 번의 무작위 초기화에 걸쳐 각 작업에 대한 중앙값 개발 세트 결과를 보고
    - 두 번째 설정(ensembles, test)에서는 GLUE 리더보드를 통해 RoBERTA를 테스트 세트의 다른 접근 방식과 비교
    - GLUE 리더 보드에 대한 많은 제출은 다중 작업 미세 조정에 의존하지만, <B>우리의 제출은 단일 작업 미세 조정에만 의존</B>
    - RTE, STS 및 MRPC의 경우 사전 훈련된 RoBERTA가 아닌 MNLI 단일 작업 모델에서 시작하는 미세 조정에 도움이 된다는 것을 발견
    - Task-specific modifications
      - GLUE 작업 중 두 가지는 경쟁력 있는 리더보드 결과를 달성하기 위해 작업별 미세 조정 접근법이 필요
      - QNLI: 최근 GLUE 리더보드에 제출된 것은 QNLI 작업에 대해 쌍 별 순위 공식을 채택하여, 이 작업에서 후보 답변이 채굴되고 서로 비교되며, 단일(질문, 후보) 쌍은 양성으로 분류
      - WNLI: 제공된 NLI 형식 데이터는 작업하기가 어려움,대신 쿼리 대명사와 참조의 범위를 나타내는 SuperGLUE의 다시 포맷된 WNLI 데이터를 사용,주어진 입력 문장의 경우, 우리는 spaCy를 사용하여 문장에서 추가 후보 명사 구를 추출하고 모델을 미세 조정하여 생성된 어떤 음의 후보 구보다 더 높은 점수를 할당

    ![image](https://user-images.githubusercontent.com/70500214/110246485-201f7600-7fab-11eb-9e3f-19a4f1799e86.png)

    - 첫 번째 설정(single-task, dev)에서 RoBERTA는 GLUD 작업 개발 세트 9개 모두에서 최첨단 결과를 달성
    - 두 번째 설정(ensembles, test)에서 우리는 GLUD 리더보드에 RoBERTA를 제출하고 9개 과제 중 4개 과제와 현재까지 가장 높은 평균 점수에 대한 최첨단 결과를 달성
    - RoBERTA가 다른 대부분의 상위 제출과 달리 다중 작업 미세 조정에 의존하지 않기 때문에 특히 흥미로움
  
   # SQuAD Results
    - 간단한 SQuAD 접근 방식을 채택
    - 모두 추가 QA 데이터 세트로 교육 데이터를 보강하지만, 제공된 SQuAD 교육 데이터를 사용하여 RoBERTa를 미세 조정할 뿐
    - 분류 및 span 손실 조건을 합하여 이 분류기를 span 예측기와 공동으로 훈련
    ![image](https://user-images.githubusercontent.com/70500214/110246700-029edc00-7fac-11eb-8c22-e1a064d318ae.png)
    
    - SQuAD v1.1 개발 세트에서 RoBERTA는 XLNet에 의해 설정된 최신 기술과 일치
    - SQuAD v2.0 개발 세트에서 RoBERTA는 XLNet에 비해 0.4 포인트(EM) 및 0.6 포인트(F1) 향상되는 새로운 최첨단 기술을 설정
    - RoBERTA를 공개 SQuAD 2.0 리더보드에 제출하고 다른 시스템과 비교하여 성능을 평가
    - 상위 시스템은 BERT 또는 XLNet을 기반으로 구축되며, 두 시스템 모두 추가 외부 교육 데이터에 의존
    - 하지만 RoBERTa는 어떠한 추가 데이터도 사용하지 않음
    - 단일 RoBERTA 모델은 단일 모델 제출 중 하나를 제외한 모든 모델보다 성능이 뛰어나며, 데이터 증대에 의존하지 않는 모델 중 최고 점수 시스템으로 나타난다.
   
   # RACE Results
    - 시스템은 텍스트, 관련 질문 및 4개의 후보 답변과 함께 제공
    - 시스템은 네 가지 후보 답변 중 어느 것이 정답인지 분류해야 함
    - 각 후보 답변을 해당 질문 및 문항과 연결하여 이 과제에 대한 RoBERTA를 수정
    - 네 가지 시퀀스를 각각 인코딩하고 완전한 연결 레이어를 통해 결과 [CLS] 표현을 전달하며, 이는 정답을 예측하는 데 사용
    - 128개의 토큰보다 긴 질문-답변 쌍을 잘라내어, 필요한 경우, 통로를 잘라내어 총 길이가 최대 512개의 토큰이 되도록 함
    
    ![image](https://user-images.githubusercontent.com/70500214/110246856-9cff1f80-7fac-11eb-8ed4-0e904cdeb242.png)
    - RoBERTA는 중학교와 고등학교 환경 모두에서 최첨단 결과를 달성
  
# Related Work
 - 사전 훈련 방법은 언어 모델링,기계 번역,마스크 언어 모델링을 포함한 다양한 교육 목표를 가지고 설계됨
 - 최근의 많은 논문은 각 최종 작업에 대해 미세 조정 모델의 기본 레시피를 사용하고 마스크된 언어 모델 목표의 일부 변형으로 사전 훈련
 - 논문의 목적은 모든 방법의 상대적 성능을 더 잘 이해하기 위한 참조 포인트로 BERT의 훈련을 복제, 단순화하고 더 잘 조정하는 것

# Conclusion
 - 더 많은 데이터에 비해 더 큰 배치를 사용하여 모델을 더 오래 훈련시키고, 다음 문장 예측 목표를 제거하고, 더 긴 시퀀스에 대한 훈련하며, 훈련 데이터에 적용되는 마스킹 패턴을 동적으로 변경함으로써 성능이 크게 향상될 수 있다는 것을 발견
 - 결과는 이전에 간과된 이러한 설계 결정의 중요성을 보여주고 BERT의 사전 훈련 목표가 최근에 제안된 대안들과 여전히 경쟁적이라는 것을 시사



   
   




